{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from unidecode import unidecode_expect_nonascii, unidecode\n",
    "client = MongoClient(connect=False)\n",
    "db = client['newscraper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo service mongod start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_schema(table='articles_cleaned'):\n",
    "    from pprint import pprint\n",
    "    pprint(next(db[table].find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5a2730f35cedcc6022e9026e'),\n",
      " 'flags': ['left-center', 'very high'],\n",
      " 'source': 'https://brookings.edu',\n",
      " 'text': 'A chronicle of the year that changed Soviet Russia—and molded the '\n",
      "         'future path of one of America’s pre-eminent diplomatic '\n",
      "         'correspondents\\n'\n",
      "         '\\n'\n",
      "         '1956 was an extraordinary year in modern Russian history. It was '\n",
      "         'called “the year of the thaw”—a time when Stalin’s dark legacy of '\n",
      "         'dictatorship died in February only to be reborn later that December. '\n",
      "         'This historic arc from rising hope to crushing despair opened with a '\n",
      "         'speech by Nikita Khrushchev, then the unpredictable leader of the '\n",
      "         'Soviet Union. He astounded everyone by denouncing the one figure '\n",
      "         'who, up to that time, had been hailed as a “genius,” a wizard of '\n",
      "         'communism—Josef Stalin himself. Now, suddenly, this once '\n",
      "         'unassailable god was being portrayed as a “madman” whose '\n",
      "         'idiosyncratic rule had seriously undermined communism and endangered '\n",
      "         'the Soviet state.\\n'\n",
      "         '\\n'\n",
      "         'This amazing switch from hero to villain lifted a heavy overcoat of '\n",
      "         'fear from the backs of ordinary Russians. It also quickly led to '\n",
      "         'anti-communist uprisings in Eastern Europe, none more bloody and '\n",
      "         'challenging than the one in Hungary, which Soviet troops crushed at '\n",
      "         'year’s end.\\n'\n",
      "         '\\n'\n",
      "         'Marvin Kalb, then a young diplomatic attaché at the U.S. Embassy in '\n",
      "         'Moscow, observed this tumultuous year that foretold the end of '\n",
      "         'Soviet communism three decades later. Fluent in Russian, a doctoral '\n",
      "         'candidate at Harvard, he went where few other foreigners would dare '\n",
      "         'go, listening to Russian students secretly attack communism and '\n",
      "         'threaten rebellion against the Soviet system, traveling from one end '\n",
      "         'of a changing country to the other and, thanks to his diplomatic '\n",
      "         'position, meeting and talking with Khrushchev, who playfully '\n",
      "         'nicknamed him Peter the Great.\\n'\n",
      "         '\\n'\n",
      "         'In this, his fifteenth book, Kalb writes a fascinating eyewitness '\n",
      "         'account of a superpower in upheaval and of a people yearning for an '\n",
      "         'end to dictatorship.',\n",
      " 'title': 'The Year I Was Peter the Great',\n",
      " 'url': 'https://www.brookings.edu/book/the-year-i-was-peter-the-great/'}\n"
     ]
    }
   ],
   "source": [
    "show_schema('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "class Corpus:\n",
    "    ''' Retrieves data from MongoDB'''\n",
    "\n",
    "    def __init__(self, db_table='articles', field='text', n_words=20000):\n",
    "\n",
    "        self.n_words = n_words\n",
    "        self.field = field\n",
    "        self.db_table = db_table\n",
    "        self.labels = [\n",
    "            'center', 'conspiracy', 'extreme left', 'extreme right',\n",
    "            'fake news', 'hate', 'high', 'left', 'left-center', 'low', 'mixed',\n",
    "            'pro-science', 'propaganda', 'right', 'right-center', 'satire',\n",
    "            'very high'\n",
    "        ]\n",
    "\n",
    "    def get_all_rows(self):\n",
    "        ''' Retrieve target table from db '''\n",
    "        print(self.n_words)\n",
    "        self.articles = [_ for _ in db[self.db_table].find()\n",
    "                         if _[self.field]]\n",
    "        self.n_articles = len(self.articles)\n",
    "\n",
    "\n",
    "from keras.preprocessing import text as Text\n",
    "\n",
    "\n",
    "class KerasVectorizer(Corpus):\n",
    "    ''' Performs vectorization and text preprocessing '''\n",
    "\n",
    "    def __init__(self, dnn_type='seq', max_len=1000, predict_str=False):\n",
    "        super().__init__()\n",
    "        if not predict_str:\n",
    "            self.get_all_rows()\n",
    "            self.train = True\n",
    "        else:\n",
    "            self.articles = predict_str\n",
    "            self.train = False\n",
    "        self.dnn_type = dnn_type\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def clean(self, seq):\n",
    "        if len(seq):\n",
    "            seq = unidecode(seq)\n",
    "            return ' '.join(\n",
    "                Text.text_to_word_sequence(\n",
    "                    seq,\n",
    "                    filters=\n",
    "                    '''1234567890!\"#$%&()*+,-\\n./—:;<=>?@[\\\\]^_`{|}~\\t\\'“”'''))\n",
    "\n",
    "    def fit(self):\n",
    "        ''' Fit vectorizer on corpus '''\n",
    "\n",
    "        Tokenizer = Text.Tokenizer\n",
    "        tokenizer = Tokenizer(self.n_words)\n",
    "\n",
    "        print('cleaning text')\n",
    "        texts = [self.clean(entry[self.field]) for entry in self.articles]\n",
    "        print('fitting vector')\n",
    "        try:\n",
    "            tokenizer = pickle.load(open('vector234.pkl', 'rb'))\n",
    "        except FileNotFoundError:\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            pickle.dump(tokenizer, open('vector234.pkl', 'wb'))\n",
    "        self.corpus_vector = tokenizer\n",
    "        self.lookup = {\n",
    "            k: v\n",
    "            for k, v in self.corpus_vector.word_index.items()\n",
    "            if v < self.n_words\n",
    "        }\n",
    "\n",
    "        json.dump(self.lookup, open('lookup234.json', 'w'))\n",
    "\n",
    "    def gen_x_onehot(self):\n",
    "        if self.train:\n",
    "            text = [self.clean(_[self.field]) for _ in self.articles]\n",
    "        else:\n",
    "            text = self.articles\n",
    "        for entry in text:\n",
    "            entry = keras.preprocessing.text.text_to_word_sequence(entry)\n",
    "            yield [self.lookup[word] for word in entry if word in self.lookup]\n",
    "\n",
    "    def transform_x_onehot(self):\n",
    "        x = list(self.gen_x_onehot())\n",
    "        #         v_len = max([len(_)for _ in x])\n",
    "        #         print ('longest text', v_len)\n",
    "        #         if v_len > self.max_len:\n",
    "        #             v_len = self.max_len\n",
    "        self.rev_lookup = {v: k for k, v in self.lookup.items()}\n",
    "        v_len = self.max_len\n",
    "        print('using limit of', v_len)\n",
    "        self.lens = []\n",
    "        for entry in x:\n",
    "            self.lens.append(len(entry))\n",
    "\n",
    "            if len(entry) >= v_len:\n",
    "                yield np.array(entry[-v_len:])\n",
    "            else:\n",
    "                yield np.array([0 for _ in range(v_len - len(entry))] + entry)\n",
    "\n",
    "    def transform_y(self):\n",
    "        ''' Vectorizes y labels '''\n",
    "        for entry in self.articles:\n",
    "            yield np.array(\n",
    "                [1 if _ in entry['flags'] else 0 for _ in self.labels])\n",
    "\n",
    "    def transform_x(self):\n",
    "        ''' Transforms texts to the vector '''\n",
    "\n",
    "        text = [self.clean(_[self.field]) for _ in self.articles]\n",
    "        return self.corpus_vector.texts_to_matrix(text)\n",
    "\n",
    "\n",
    "#         vector = pickle.load(open('./vector234.pkl', 'rb'))\n",
    "\n",
    "#         self.lookup = json.load(open('lookup234.json'))\n",
    "\n",
    "#         return list(self.transform_x_onehot())\n",
    "\n",
    "    def x_y(self):\n",
    "        self.fit()\n",
    "        print('producing x, y data')\n",
    "        y = list(self.transform_y())\n",
    "\n",
    "        if self.dnn_type == 'seq':\n",
    "            x = list(self.transform_x_onehot())\n",
    "        elif self.dnn_type == 'bow':\n",
    "            x = self.transform_x()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    k_v = KerasVectorizer(max_len=2000)\n",
    "    #http://www.newswhip.com/2013/12/article-length/\n",
    "    x, y = k_v.x_y()\n",
    "    print('data prepared')\n",
    "    print(x[0].shape)\n",
    "\n",
    "    return k_v, x, y\n",
    "\n",
    "\n",
    "def predict_data(text):\n",
    "    k_v = KerasVectorizer(max_len=2000, predict_str=[text])\n",
    "\n",
    "    x = k_v.transform_x()\n",
    "    print('data prepared')\n",
    "    print(x[0].shape)\n",
    "\n",
    "    return k_v, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm *.pkl\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickles loaded\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "\n",
    "def train_setup():\n",
    "    k_v, X, Y = prep_data()\n",
    "\n",
    "    def val_set(x, y):\n",
    "        val_size = .15\n",
    "        val_ind = int(len(x) * val_size)\n",
    "        print(val_ind, len(x))\n",
    "\n",
    "        randomize = np.arange(len(x))\n",
    "        np.random.shuffle(randomize)\n",
    "\n",
    "        x = np.array(x)[randomize]\n",
    "        y = np.array(y)[randomize]\n",
    "\n",
    "        x = x[:-val_ind]\n",
    "        y = y[:-val_ind]\n",
    "        x_val = x[-val_ind:]\n",
    "        y_val = y[-val_ind:]\n",
    "        assert len(y) == len(x)\n",
    "\n",
    "        return x, y, x_val, y_val\n",
    "\n",
    "    x, y, x_val, y_val = val_set(X, Y)\n",
    "    return x, y, x_val, y_val, k_v\n",
    "\n",
    "\n",
    "def load_pickles():\n",
    "    pickle_rick = 'x', 'y', 'x_val', 'y_val', 'k_v'\n",
    "\n",
    "    for rick in pickle_rick:\n",
    "        yield pickle.load(open(rick + '.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def save_pickles():\n",
    "    x, y, x_val, y_val, k_v = train_setup()\n",
    "    print('saving pickles')\n",
    "    pickle_rick = {'x': x, 'y': y, 'x_val': x_val, 'y_val': y_val, 'k_v': k_v}\n",
    "    for k, v in pickle_rick.items():\n",
    "        yield pickle.dump(v, open(k + '.pkl', 'wb'))\n",
    "\n",
    "\n",
    "try:\n",
    "    x, y, x_val, y_val, k_v = list(load_pickles())\n",
    "    print('pickles loaded')\n",
    "except Exception as e:\n",
    "\n",
    "    #     x, y, x_val, y_val, k_v = train_setup()\n",
    "    \n",
    "    list(save_pickles())\n",
    "    print(e)\n",
    "finally:\n",
    "    x, y, x_val, y_val, k_v = list(load_pickles())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T21:13:06.033567Z",
     "start_time": "2019-05-21T21:13:06.030826Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = np.random.randint(0, len(x))\n",
    "# print(i)\n",
    "# print(x[i])\n",
    "# #print([k_v.labels[n] for n,v in enumerate(y[i]) if v >0])\n",
    "# for word in x[i]:\n",
    "#     if word:\n",
    "#         print(k_v.rev_lookup[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2000, 10)     200000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2000, 10, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2000, 1, 128) 1408        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1999, 1, 128) 2688        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1998, 1, 128) 3968        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 128)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           24640       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32)           128         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 17)           561         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 235,729\n",
      "Trainable params: 235,537\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 25266 samples, validate on 4458 samples\n",
      "Epoch 1/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.8045Epoch 00001: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 64s 3ms/step - loss: 0.4506 - acc: 0.8045 - val_loss: 0.3580 - val_acc: 0.8669\n",
      "\n",
      "Epoch 2/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8675Epoch 00002: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 64s 3ms/step - loss: 0.3581 - acc: 0.8675 - val_loss: 0.3463 - val_acc: 0.8693\n",
      "\n",
      "Epoch 3/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8709Epoch 00003: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.3455 - acc: 0.8709 - val_loss: 0.3265 - val_acc: 0.8764\n",
      "\n",
      "Epoch 4/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8750Epoch 00004: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.3316 - acc: 0.8750 - val_loss: 0.3106 - val_acc: 0.8800\n",
      "\n",
      "Epoch 5/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8779Epoch 00005: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.3199 - acc: 0.8779 - val_loss: 0.2948 - val_acc: 0.8852\n",
      "\n",
      "Epoch 6/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8819Epoch 00006: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.3065 - acc: 0.8819 - val_loss: 0.2794 - val_acc: 0.8922\n",
      "\n",
      "Epoch 7/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.8859Epoch 00007: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2940 - acc: 0.8859 - val_loss: 0.2635 - val_acc: 0.8977\n",
      "\n",
      "Epoch 8/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.8895Epoch 00008: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2821 - acc: 0.8895 - val_loss: 0.2480 - val_acc: 0.9018\n",
      "\n",
      "Epoch 9/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.8935Epoch 00009: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2712 - acc: 0.8935 - val_loss: 0.2421 - val_acc: 0.9019\n",
      "\n",
      "Epoch 10/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.8972Epoch 00010: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2611 - acc: 0.8971 - val_loss: 0.2232 - val_acc: 0.9122\n",
      "\n",
      "Epoch 11/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.8998Epoch 00011: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2528 - acc: 0.8998 - val_loss: 0.2129 - val_acc: 0.9153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9026Epoch 00012: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2447 - acc: 0.9026 - val_loss: 0.2049 - val_acc: 0.9173\n",
      "\n",
      "Epoch 13/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9054Epoch 00013: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2371 - acc: 0.9054 - val_loss: 0.1969 - val_acc: 0.9205\n",
      "\n",
      "Epoch 14/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9073Epoch 00014: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2309 - acc: 0.9074 - val_loss: 0.1893 - val_acc: 0.9230\n",
      "\n",
      "Epoch 15/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9095Epoch 00015: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2249 - acc: 0.9095 - val_loss: 0.1804 - val_acc: 0.9279\n",
      "\n",
      "Epoch 16/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9122Epoch 00016: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2186 - acc: 0.9122 - val_loss: 0.1734 - val_acc: 0.9302\n",
      "\n",
      "Epoch 17/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9137Epoch 00017: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2142 - acc: 0.9136 - val_loss: 0.1682 - val_acc: 0.9325\n",
      "\n",
      "Epoch 18/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9156Epoch 00018: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.2088 - acc: 0.9156 - val_loss: 0.1644 - val_acc: 0.9346\n",
      "\n",
      "Epoch 19/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9178Epoch 00019: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.2038 - acc: 0.9178 - val_loss: 0.1606 - val_acc: 0.9360\n",
      "\n",
      "Epoch 20/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9198Epoch 00020: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1986 - acc: 0.9198 - val_loss: 0.1524 - val_acc: 0.9413\n",
      "\n",
      "Epoch 21/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9207Epoch 00021: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1966 - acc: 0.9207 - val_loss: 0.1479 - val_acc: 0.9427\n",
      "\n",
      "Epoch 22/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9229Epoch 00022: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1915 - acc: 0.9229 - val_loss: 0.1482 - val_acc: 0.9439\n",
      "\n",
      "Epoch 23/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9241Epoch 00023: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1881 - acc: 0.9241 - val_loss: 0.1408 - val_acc: 0.9464\n",
      "\n",
      "Epoch 24/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9253Epoch 00024: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1848 - acc: 0.9253 - val_loss: 0.1366 - val_acc: 0.9478\n",
      "\n",
      "Epoch 25/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9272Epoch 00025: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 3ms/step - loss: 0.1812 - acc: 0.9272 - val_loss: 0.1336 - val_acc: 0.9474\n",
      "\n",
      "Epoch 26/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9281Epoch 00026: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1788 - acc: 0.9281 - val_loss: 0.1290 - val_acc: 0.9502\n",
      "\n",
      "Epoch 27/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9299Epoch 00027: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1751 - acc: 0.9299 - val_loss: 0.1255 - val_acc: 0.9524\n",
      "\n",
      "Epoch 28/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9319Epoch 00028: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1703 - acc: 0.9318 - val_loss: 0.1221 - val_acc: 0.9531\n",
      "\n",
      "Epoch 29/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9325Epoch 00029: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1691 - acc: 0.9325 - val_loss: 0.1216 - val_acc: 0.9552\n",
      "\n",
      "Epoch 30/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9334Epoch 00030: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1662 - acc: 0.9334 - val_loss: 0.1166 - val_acc: 0.9577\n",
      "\n",
      "Epoch 31/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9346Epoch 00031: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1640 - acc: 0.9346 - val_loss: 0.1137 - val_acc: 0.9584\n",
      "\n",
      "Epoch 32/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9351Epoch 00032: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1617 - acc: 0.9351 - val_loss: 0.1119 - val_acc: 0.9593\n",
      "\n",
      "Epoch 33/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9361Epoch 00033: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1604 - acc: 0.9361 - val_loss: 0.1098 - val_acc: 0.9602\n",
      "\n",
      "Epoch 34/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9375Epoch 00034: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1570 - acc: 0.9375 - val_loss: 0.1061 - val_acc: 0.9617\n",
      "\n",
      "Epoch 35/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9387Epoch 00035: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1554 - acc: 0.9386 - val_loss: 0.1033 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9386Epoch 00036: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1547 - acc: 0.9386 - val_loss: 0.1035 - val_acc: 0.9638\n",
      "\n",
      "Epoch 37/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9396Epoch 00037: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1521 - acc: 0.9396 - val_loss: 0.1004 - val_acc: 0.9644\n",
      "\n",
      "Epoch 38/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9409Epoch 00038: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 63s 2ms/step - loss: 0.1494 - acc: 0.9409 - val_loss: 0.0973 - val_acc: 0.9648\n",
      "\n",
      "Epoch 39/80\n",
      "17088/25266 [===================>..........]17088/25266 [===================>..........] - ETA: 19s - loss: 0.1454 - acc: 0.9425"
     ]
    }
   ],
   "source": [
    "def dnn():\n",
    "\n",
    "    Sequential = keras.models.Sequential\n",
    "    load_model = keras.models.load_model\n",
    "    Tokenizer = keras.preprocessing.text.Tokenizer\n",
    "    Activation = keras.layers.Activation\n",
    "    SGD = keras.optimizers.SGD\n",
    "    Adam = keras.optimizers.Adam\n",
    "    BatchNormalization = keras.layers.BatchNormalization\n",
    "    to_categorical = keras.utils.to_categorical\n",
    "    ModelCheckpoint = keras.callbacks.ModelCheckpoint\n",
    "    Embedding = keras.layers.Embedding\n",
    "    Reshape = keras.layers.Reshape\n",
    "    Flatten = keras.layers.Flatten\n",
    "    Dropout = keras.layers.Dropout\n",
    "    Concatenate = keras.layers.Concatenate\n",
    "    Dense = keras.layers.Dense\n",
    "    Model = keras.models.Model\n",
    "    Input = keras.layers.Input\n",
    "    Conv2D = keras.layers.Conv2D\n",
    "    MaxPool2D = keras.layers.MaxPool2D\n",
    "    Conv1D = keras.layers.Conv1D\n",
    "    MaxPool1D = keras.layers.MaxPool1D\n",
    "\n",
    "    n_classes = 17\n",
    "\n",
    "    def define_model_rnn():\n",
    "        vector_len = x[0].shape[0]\n",
    "        vocab_size = k_v.n_words\n",
    "        embedding_dim = 10\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            keras.layers.Embedding(\n",
    "                vocab_size, embedding_dim, input_shape=(vector_len, )))\n",
    "        model.add(keras.layers.GRU(3, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes, ))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def define_model():\n",
    "        vector_len = k_v.n_words\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(vector_len, )))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(n_classes, ))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def define_model_cnn():\n",
    "\n",
    "        sequence_length = x.shape[1]\n",
    "        vocabulary_size = k_v.n_words\n",
    "        embedding_dim = 10\n",
    "        filter_sizes = [1,2, 3]\n",
    "        num_filters = 128\n",
    "        drop = 0.5\n",
    "        batch_size = 5\n",
    "\n",
    "        inputs = Input(shape=(sequence_length, ), dtype='int32')\n",
    "        \n",
    "        embedding = Embedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length)(inputs)\n",
    "        \n",
    "        reshape = Reshape((sequence_length, embedding_dim, 1))(embedding)\n",
    "\n",
    "        conv_0 = Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(filter_sizes[0], embedding_dim),\n",
    "            padding='valid',\n",
    "            kernel_initializer='normal',\n",
    "            activation='relu')(reshape)\n",
    "        conv_1 = Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(filter_sizes[1], embedding_dim),\n",
    "            padding='valid',\n",
    "            kernel_initializer='normal',\n",
    "            activation='relu')(reshape)\n",
    "        \n",
    "        conv_2 = Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(filter_sizes[2], embedding_dim),\n",
    "            padding='valid',\n",
    "            kernel_initializer='normal',\n",
    "            activation='relu')(reshape)\n",
    "\n",
    "        maxpool_0 = MaxPool2D(\n",
    "            pool_size=(sequence_length - filter_sizes[0] + 1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='valid')(conv_0)\n",
    "        maxpool_1 = MaxPool2D(\n",
    "            pool_size=(sequence_length - filter_sizes[1] + 1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='valid')(conv_1)\n",
    "        \n",
    "        maxpool_2 = MaxPool2D(\n",
    "            pool_size=(sequence_length - filter_sizes[2] + 1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='valid')(conv_2)\n",
    "\n",
    "        concatenated_tensor = Concatenate(axis=1)(\n",
    "            [maxpool_0, maxpool_1,maxpool_2])\n",
    "\n",
    "        flatten = Flatten()(concatenated_tensor)\n",
    "        dropout = Dropout(drop)(flatten)\n",
    "        dense1= Dense(64, activation='relu')(dropout)\n",
    "        bnorm1 = BatchNormalization()(dense1)\n",
    "        dense2= Dense(32, activation='relu')(bnorm1)\n",
    "        bnorm2 = BatchNormalization()(dense2)\n",
    "        output = Dense(units=n_classes, activation='sigmoid')(bnorm2)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        print (model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "    label_dict = {k: i for i, k in enumerate(k_v.labels)}\n",
    "\n",
    "    print('starting training')\n",
    "\n",
    "    def train():\n",
    "\n",
    "        model = define_model_cnn()\n",
    "\n",
    "        embedding_layer_names = set(\n",
    "            layer.name for layer in model.layers\n",
    "            if layer.name.startswith('embedding_')\n",
    "            or layer.name.startswith('dense_'))\n",
    "\n",
    "        #         tb = keras.callbacks.TensorBoard(\n",
    "        #             histogram_freq=0,\n",
    "        #             batch_size=30,\n",
    "        #             log_dir='./logs/test',\n",
    "        #             write_graph=False,\n",
    "        #             write_grads=False,\n",
    "        #             write_images=False,\n",
    "        #             embeddings_freq = 1,\n",
    "        #             embeddings_layer_names=embedding_layer_names,\n",
    "        #             embeddings_metadata='metadata.tsv')\n",
    "\n",
    "        lr1 = Adam(lr=0.00005)\n",
    "        lr2 = Adam(lr=0.0001)\n",
    "        adam = Adam(lr=0.001)\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto')\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath='tester.h5', verbose=1, save_best_only=False)\n",
    "        \n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        history = model.fit(\n",
    "            np.array(x),\n",
    "            np.array(y),\n",
    "            epochs=80,\n",
    "            verbose=1,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[\n",
    "                #                 tb,\n",
    "                keras.callbacks.TensorBoard(\n",
    "                    log_dir='./logs/CNN', write_graph=False),\n",
    "                early_stop,\n",
    "                checkpointer,\n",
    "            ])\n",
    "\n",
    "    train()\n",
    "\n",
    "\n",
    "dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "load_model = keras.models.load_model\n",
    "model = load_model('tester.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_dict = {i: k for i, k in enumerate(k_v.labels)}\n",
    "\n",
    "preds = [model.predict(np.array(text).reshape(1,-1)) for text in x_val[:10]]\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "preds\n",
    "pred_dict = {\n",
    "    label_dict[i]: round(float(p), 6) for i, p in enumerate([_ for _ in preds[0].flatten()])\n",
    "}\n",
    "\n",
    "\n",
    "final_output = [    {\n",
    "    label_dict[i]: round(float(p), 6) for i, p in enumerate([_ for _ in pred.flatten()])\n",
    "} for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i in range(15):\n",
    "    t = [\n",
    "        _[0] for _ in sorted(\n",
    "            final_output[i].items(), key=lambda kv: kv[1], reverse=True)[:1]\n",
    "    ]\n",
    "\n",
    "    p = [k_v.labels[j] for j in [k for k, _ in enumerate(y_val[i]) if _ > 0]]\n",
    "    print(t, '\\t', p,len(set(p) & set(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "res = pd.DataFrame(final_output,y_val[:10])\n",
    "res.transpose().plot(kind='barh');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def metadata():\n",
    "    with open('./logs/test/metadata.tsv','w') as meta:\n",
    "        meta.write('word\\tvalue\\n')\n",
    "        \n",
    "        meta.write('NULL\\tNULL\\n')\n",
    "        for k, v in sorted(k_v.lookup.items(),key=lambda kv: kv[1]):\n",
    "            meta.write(k+'\\t'+str(v)+'\\n')\n",
    "        \n",
    "    with open('./logs/test/metadata.tsv') as meta_read:\n",
    "        print(len([_ for _ in meta_read.readlines()]))\n",
    "#         print(meta_read.read()[:100])\n",
    "        return\n",
    "        \n",
    "\n",
    "        \n",
    "def labels():\n",
    "    with open('./logs/test/metadata_labels.tsv','w') as meta:\n",
    "        meta.write('label\\tnumber\\n')\n",
    "        for k, v in enumerate(k_v.labels):\n",
    "            \n",
    "            \n",
    "            meta.write(str(k)+'\\t'+str(v))\n",
    "                \n",
    "            meta.write('\\n')\n",
    "                    \n",
    "            \n",
    "        \n",
    "    with open('./logs/test/metadata_labels.tsv') as meta_read:\n",
    "        print(meta_read.read())\n",
    "#         print(len([_ for _ in meta_read.readlines()]))\n",
    "        \n",
    "        return\n",
    "metadata()\n",
    "# labels()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize():\n",
    "    rank = (_ for _ in k_v.lookup.items())\n",
    "    [next(rank) for _ in range(18999)]\n",
    "    pprint([next(rank) for _ in range(1000)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
